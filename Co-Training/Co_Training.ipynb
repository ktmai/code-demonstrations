{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9yda7NTTFxL"
   },
   "source": [
    "Implementation of the co-training algorithm proposed by Blum and Mitchell (1998).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOJvzjktTCJ1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z156i7FarUXr"
   },
   "source": [
    "The data consists of HTML scraped from the webpages. As we're interested in the text contents only, we will clean this up by removing the HTML tags, converting all the words to lower case (as proper case and lower case words will be treated the same), and remove 'stop words' like 'the' which are commonly occuring words but probably do not contribute much to the classifier. We will also replace numeric values with 'num' as these are likely to relate to different courses. Much of the help came from this [Cambridge Spark](https://https://blog.cambridgespark.com/tutorial-preprocessing-text-data-a8969189b779) tutorial.\n",
    "\n",
    "The original paper uses a bag of words representation, so we will do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QakiAXTpq5o"
   },
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "  text_array = []\n",
    "  # Load all of the files for each view\n",
    "  for path, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "      fname = os.path.join(path, filename)\n",
    "      with open(fname, 'rb') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        # Remove the HTML tags and convert all to lowercase as case does not matter\n",
    "        text_array.append(soup.get_text().lower())\n",
    "  text_array = [re.sub(r'\\d+', 'num', page) for page in text_array]\n",
    "  return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QK-wL7-FXYS-"
   },
   "outputs": [],
   "source": [
    "def process_data(text_array):\n",
    "  counter = CountVectorizer(stop_words='english')\n",
    "  bag_of_words = counter.fit_transform(text_array).toarray()\n",
    "  return counter, bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmVWqpmTsZ_S"
   },
   "source": [
    "We will create accompanying labels, setting course to '1' and non-course to '0' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBAgFPFooEWv"
   },
   "outputs": [],
   "source": [
    "num_course = np.ones((230,1))\n",
    "num_noncourse = np.zeros((821,1))\n",
    "labels = np.vstack((num_course, num_noncourse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cpj-LwQQWhA1"
   },
   "outputs": [],
   "source": [
    "# Load data from both views\n",
    "fulltext_dir ='course-cotrain-data/fulltext/'\n",
    "inlinks_dir = 'course-cotrain-data/inlinks/'\n",
    "\n",
    "fulltext_data = load_data(fulltext_dir)\n",
    "inlinks_data = load_data(inlinks_dir)\n",
    "\n",
    "# Create BoW representations for both views\n",
    "f_counter, f_bow = process_data(fulltext_data)\n",
    "i_counter, i_bow = process_data(inlinks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GyUeT_Yqmk9"
   },
   "source": [
    "# Comparison: train a naive Bayes classifier on a single view\n",
    "\n",
    "For comparison, we will compare the co-training method to naive Bayes classifiers trained on a single view only. Note that the below setting is supervised and not semi-supervised.\n",
    "\n",
    "As the dataset is rather imbalanced as there are more non-course samples than course samples, accuracy would not be a reliable metric. Therefore we will use the F1-score which is a weighted average of precision ($\\frac{\\textrm{true positive}}{\\textrm{true positive}+\\textrm{false positive}}$) and recall ($\\frac{\\textrm{true positive}}{\\textrm{true positive}+\\textrm{false negative}}$).\n",
    "\n",
    "$$F_1 = \\frac{2\\times\\textrm{precision}\\times\\textrm{recall}}{\\textrm{precision}+\\textrm{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTBs64lgsoqS"
   },
   "outputs": [],
   "source": [
    "# For comparision, we will make the same split for each classifier\n",
    "f_train, f_test, i_train, i_test, y_train, y_test = train_test_split(f_bow, i_bow, labels.ravel(), test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9h64SpmyqvVI"
   },
   "source": [
    "## Fulltext data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "RLwshXlDaMdh",
    "outputId": "2d4b6a63-28f2-4700-c0df-fa227d2f364b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[166  30]\n",
      " [ 48  19]] \n",
      " F1 Score: 0.3275862068965517\n"
     ]
    }
   ],
   "source": [
    "# Initialise a naive Bayes classifier\n",
    "f_gnb = MultinomialNB()\n",
    "\n",
    "# Train and predict on the test set\n",
    "f_pred = f_gnb.fit(f_train, y_train).predict(f_test)\n",
    "\n",
    "# Print the confusion matrix and F1 score\n",
    "print(\"Confusion matrix: \\n {} \\n F1 Score: {}\".format(confusion_matrix(y_test, f_pred),f1_score(y_test, f_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7A9E2aMgsRkM"
   },
   "source": [
    "## Inlinks data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "eft7wu4arQe6",
    "outputId": "8dfe4718-89e6-4749-e5db-520da0191e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[100  96]\n",
      " [ 32  35]] \n",
      " F1 Score: 0.35353535353535354\n"
     ]
    }
   ],
   "source": [
    "i_gnb = MultinomialNB()\n",
    "\n",
    "i_pred = i_gnb.fit(i_train, y_train).predict(i_test)\n",
    "\n",
    "print(\"Confusion matrix: \\n {} \\n F1 Score: {}\".format(confusion_matrix(y_test, i_pred),f1_score(y_test, i_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftZfU_1Escs5"
   },
   "source": [
    "# Co-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTq0trjivCN6"
   },
   "source": [
    "The co-training paper uses 25% of the data as the test data and the remaining as the training data. Within the training data, we generate three subsets: $L$ (the labelled data), $U$ (the unlabelled data), and $U'$ (a subset of $U$). \n",
    "\n",
    "Initially $L$ comprises of 3 positive (course) and 9 negative (non-course) samples. This is generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7jaASji-JOq"
   },
   "outputs": [],
   "source": [
    "L_i_positive = i_train[np.where(y_train ==1)[0][:3],:]\n",
    "L_i_negative = i_train[np.where(y_train != 1)[0][:9],:]\n",
    "L_i = np.vstack((L_i_positive, L_i_negative))\n",
    "\n",
    "# This is the complement of L_i\n",
    "U_i = np.vstack((i_train[np.where(y_train ==1)[0][3:],:], i_train[np.where(y_train != 1)[0][9:],:]))\n",
    "\n",
    "L_f_positive = f_train[np.where(y_train ==1)[0][:3],:]\n",
    "L_f_negative = f_train[np.where(y_train != 1)[0][:9],:]\n",
    "L_f = np.vstack((L_f_positive, L_f_negative))\n",
    "\n",
    "# This is the complement of L_f\n",
    "U_f = np.vstack((f_train[np.where(y_train ==1)[0][3:],:], f_train[np.where(y_train != 1)[0][9:],:]))\n",
    "\n",
    "# Create the labels\n",
    "L_labels = np.vstack((np.ones((3,1)), np.zeros((9,1)))).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzpTPoAYv--W"
   },
   "source": [
    "We now implement the co-training algorithm. As we are continually adding samples to $L$, training the Naive Bayes classifier directly consumes increasingly more memory. Therefore we have to compromise and train in randomly sampled batches using a partial fit, meaning that the number of iterations for which the classifier is trained is different to what is specified in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ha7YiBnQeRve"
   },
   "outputs": [],
   "source": [
    "# Function to generate U' from random samples of U\n",
    "def sample_U(U_i, U_f, u = 75):\n",
    "  # We use the random indices to pick random samples\n",
    "  random_indices = np.arange(0, U_i.shape[0])\n",
    "  np.random.shuffle(random_indices)\n",
    "  U_i_prime = U_i[random_indices[:u],:]\n",
    "  U_f_prime = U_f[random_indices[:u],:]\n",
    "\n",
    "  # U is now the complement of U'\n",
    "  U_i = U_i[random_indices[:u],:]\n",
    "  U_f = U_f[random_indices[:u],:]\n",
    "\n",
    "  return U_i_prime, U_f_prime, U_i, U_f\n",
    "\n",
    "# Function which updates L by adding the most confident positive and negative values from each classifier\n",
    "def label_from_U(U_f_prime, U_i_prime, f_scores, i_scores, L_f, L_i, L_labels, p = 1, n = 3):\n",
    "\n",
    "  f_scores_pos = np.argsort(f_scores[:,1], axis=0)[::-1][:p]\n",
    "  f_scores_neg = np.argsort(f_scores[:,0], axis=0)[::-1][:n]\n",
    "  i_scores_pos = np.argsort(i_scores[:,1], axis=0)[::-1][:p]\n",
    "  i_scores_neg = np.argsort(i_scores[:,0], axis=0)[::-1][:n]\n",
    "\n",
    "  # Get the complement of the above scores so that U' can be updated. Needs to include predictions from both classifiers\n",
    "  scores_pos = np.union1d(np.argsort(f_scores[:,1], axis=0)[::-1][p:], np.argsort(i_scores[:,1], axis=0)[::-1][p:])\n",
    "  scores_neg = np.union1d(np.argsort(f_scores[:,0], axis=0)[::-1][n:], np.argsort(i_scores[:,0], axis=0)[::-1][n:])\n",
    "\n",
    "  f_pos = U_f_prime[f_scores_pos,:]\n",
    "  f_neg = U_f_prime[f_scores_neg,:]\n",
    "  fi_pos = U_i_prime[f_scores_pos,:]\n",
    "  fi_neg = U_i_prime[f_scores_neg,:]\n",
    "  i_pos = U_i_prime[i_scores_pos,:]\n",
    "  i_neg = U_i_prime[i_scores_neg,:]\n",
    "  if_pos = U_f_prime[i_scores_pos,:]\n",
    "  if_neg = U_f_prime[i_scores_neg,:]\n",
    "\n",
    "  # Add most confident values from both classifiers to L\n",
    "  L_f = np.vstack((L_f, f_pos, if_pos, f_neg, if_neg))\n",
    "  L_i = np.vstack((L_i, i_pos, fi_pos, i_neg, fi_neg))\n",
    "\n",
    "  # Update labels\n",
    "  L_labels = np.vstack((L_labels[:,np.newaxis], np.ones((2*p,1)), np.zeros((2*n,1)))).ravel()\n",
    "\n",
    "  return L_f, L_i, L_labels, scores_pos, scores_neg\n",
    "\n",
    "# Function to sample from L\n",
    "def batch_sample(L_f, L_i, L_labels, sample_size = 16):\n",
    "  random_indices = np.arange(0, L_f.shape[0])\n",
    "  np.random.shuffle(random_indices)\n",
    "  batch_f = L_f[random_indices[:sample_size],:]\n",
    "  batch_i = L_i[random_indices[:sample_size],:]\n",
    "  batch_labels = L_labels[random_indices[:sample_size]]\n",
    "\n",
    "  return batch_f, batch_i, batch_labels\n",
    "\n",
    "def cotrain(L_i, U_i, L_f, U_f, L_labels, f_test, i_test, y_test, p = 1, n = 3, k = 30, u = 75):\n",
    "  # Generate U'\n",
    "  U_i_prime, U_f_prime, U_i, U_f = sample_U(U_i, U_f, u = u)\n",
    "\n",
    "  # Train classifiers on the original L\n",
    "  # Train h1\n",
    "  f_gnb = MultinomialNB()\n",
    "  # Get the confidence scores for h1\n",
    "  f_scores = f_gnb.fit(L_f, L_labels).predict_proba(U_f_prime)\n",
    "  # Train h2\n",
    "  i_gnb = MultinomialNB()\n",
    "  # Get  the confidence scores for h2\n",
    "  i_scores = i_gnb.fit(L_i, L_labels).predict_proba(U_i_prime)\n",
    "\n",
    "  L_f, L_i, L_labels, _, _ = label_from_U(U_f_prime, U_i_prime, f_scores, i_scores, L_f, L_i, L_labels)\n",
    "\n",
    "  # Loop for k iterations\n",
    "  for count in range(k - 1):\n",
    "    batch_f, batch_i, batch_labels = batch_sample(L_f, L_i, L_labels)\n",
    "    # Partially fit h1 and h2 using the batch\n",
    "    f_scores = f_gnb.partial_fit(batch_f, batch_labels).predict_proba(U_f_prime)\n",
    "    i_scores = i_gnb.partial_fit(batch_i, batch_labels).predict_proba(U_i_prime)\n",
    "    # At the final iteration, record the final predictions which will be used to measure performance\n",
    "    if count == k - 2:\n",
    "      f_pred = f_gnb.predict(f_test)\n",
    "      i_pred = i_gnb.predict(i_test)\n",
    "    L_f, L_i, L_labels, scores_pos, scores_neg = label_from_U(U_f_prime, U_i_prime, f_scores, i_scores, L_f, L_i, L_labels)\n",
    "\n",
    "    # Remove labelled elements from U'\n",
    "    U_f_prime = np.vstack((U_f_prime[scores_pos,:], U_f_prime[scores_neg,:]))\n",
    "    U_i_prime = np.vstack((U_i_prime[scores_pos,:], U_i_prime[scores_neg,:]))\n",
    "\n",
    "    # Replenish U'\n",
    "    num_new_samples = 2*p + 2*n\n",
    "    U_i_prime, U_f_prime, U_i, U_f = sample_U(U_i, U_f, u = num_new_samples)\n",
    "\n",
    "  return f_pred, i_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uki2j_fziDGu"
   },
   "outputs": [],
   "source": [
    "f_pred, i_pred = cotrain(L_i, U_i, L_f, U_f, L_labels, f_test, i_test, y_test, k =300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAqK_41_4_SU"
   },
   "source": [
    "The results are more variable as we are using batches but there is a slight improvement in the F1 score for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5JHVQw8Pk4DH",
    "outputId": "163ed649-7dd9-4ec4-da74-56cbe188eccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[123  73]\n",
      " [ 35  32]] \n",
      " F1 Score: 0.37209302325581395\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix and F1 score for the co-trained classifer trained on fulltext\n",
    "print(\"Confusion matrix: \\n {} \\n F1 Score: {}\".format(confusion_matrix(y_test, f_pred),f1_score(y_test, f_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "MnuekU2Ok4lN",
    "outputId": "bd2ed389-3ec2-4667-970a-5516eaa0c04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[ 47 149]\n",
      " [  8  59]] \n",
      " F1 Score: 0.4290909090909091\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix and F1 score for the co-trained classifer trained on inlinks\n",
    "print(\"Confusion matrix: \\n {} \\n F1 Score: {}\".format(confusion_matrix(y_test, i_pred),f1_score(y_test, i_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Co-Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
